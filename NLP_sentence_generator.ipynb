{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLI_NLP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iXYbBcAPfUyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fundamentals of Deep Learning for Natural language Processing\n",
        "Hyungon Ryu, Sr. Solutions Architect hryu@nvidia.com \n"
      ]
    },
    {
      "metadata": {
        "id": "1nxtKtRkfpND",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "NLP is all about creating system that process language in order to perform certain tasks including \n",
        "- Answering question from Assistant such as Apple Siri, Amazon Alexa,  Microsoft Cortana and Samsung Bixby\n",
        "- Sentimenta analysis from news papers or any text determining whether a sentence or phrase has a positive or negative.\n",
        "- Image captioning  by gathering information from image and generate sentence similar as Microsoft COCO dataset image annotations. \n",
        "- Gathering  the information from text and generate summarizing sentence is also good example of NLP \n",
        "- Machine Translation is also huge domain translating a paragraph of test to another language and there are other many NLP areas .  \n"
      ]
    },
    {
      "metadata": {
        "id": "pdtjtpomf0dA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For NLP, we make word vector to represent word in n dimension vectors. below is one example. \n",
        "\n",
        "```\n",
        "NVIDIA    = [00010000] \n",
        "Deep      = [10100000]\n",
        "Learning  = [10001000]\n",
        "```\n",
        "There are many  word embedding technique and look at the word2vec algorithm. \n",
        "\n",
        "With large input corpus would produce a vector space and each unique word in the corpus being assigned a corresponding vector in the space. After training, word vectors are positioned in the vector space sharing common contexts in the corpus are located in close to one another in the space.  \n",
        "\n",
        "Moreover, Word2Vector was the appearance of linear relationship between different word vectors.  the word vectors seemed to capture different grammatical and semantic concepts. For example, we could represent vector operation such as Queen is King minus man  plus woman. \n"
      ]
    },
    {
      "metadata": {
        "id": "ZjkWuM3ofTek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "20ee2722-3cea-46ae-d3d1-ca0856a9b9d5"
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 19 23:46:21 2018       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    29W / 149W |      0MiB / 11439MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1te6X-QfBsb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "this is critical option. TF memory control allow_growth is mandatory option\n",
        "'''\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvPFMehPfFvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8abd0e0d-761a-485c-dcde-374654612618"
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi   | grep MiB"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| N/A   70C    P0    69W / 149W |    115MiB / 11439MiB |      0%      Default |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0HTv1k4me5me",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "''' from keras example \n",
        "Example script to generate text from Nietzsche's writings.\n",
        "\n",
        "At least 20 epochs are required before the generated text\n",
        "starts sounding coherent.\n",
        "\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "\n",
        "If you try this script on new data, make sure your corpus\n",
        "has at least ~100k characters. ~1M is better.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import io\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zuOnTzjCZBeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "4e75abeb-c7c0-4864-ff8c-00e5e165381d"
      },
      "cell_type": "code",
      "source": [
        "%%time \n",
        "from keras.utils import get_file\n",
        "path = get_file('nietzsche.txt',     origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "#path = get_file('trump-twittter.txt', origin='https://raw.githubusercontent.com/yhgon/SMWU_DL/master/trump-twittter.txt')\n",
        "!head -n 2 {path} && tail -n 2 {path}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 1us/step\n",
            "PREFACE\n",
            "\n",
            "Buddhists as essential to sanctity, just as they were denounced by the\n",
            "christian world as the indications of sinfulness.CPU times: user 47.8 ms, sys: 26.9 ms, total: 74.7 ms\n",
            "Wall time: 2.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y3S8h8HAfJX_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "cdb1aa94-1226-454c-e2c2-11d3d9d77d72"
      },
      "cell_type": "code",
      "source": [
        "%%time \n",
        "\n",
        "with io.open(path, encoding='utf-8') as f:\n",
        "    text = f.read().lower()\n",
        "print('corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('nb sequences:', len(sentences))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 600893\n",
            "total chars: 57\n",
            "nb sequences: 200285\n",
            "Vectorization...\n",
            "CPU times: user 3.05 s, sys: 258 ms, total: 3.3 s\n",
            "Wall time: 3.31 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XXwPLUupc6K0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Activation, Dropout, Flatten, BatchNormalization, Embedding\n",
        "from keras.objectives import MSE, MAE\n",
        "from keras.optimizers import RMSprop, adam\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping\n",
        "\n",
        "K.set_session(K.tf.Session(config=config))\n",
        "\n",
        "K.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vPUf1oXl8Kd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "acfec456-85a5-4627-e364-b6e964f79102"
      },
      "cell_type": "code",
      "source": [
        "# build the model: a single LSTM\n",
        "K.clear_session()\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add( LSTM( 64 , input_shape=(maxlen, len(chars))  , return_sequences=True, activation='tanh') )\n",
        "model.add( Dropout(0.10))\n",
        "model.add( BatchNormalization( ) )\n",
        "model.add( LSTM( 64 , activation='relu' ) )\n",
        "model.add( Dropout(0.10))\n",
        "model.add( BatchNormalization( ) )\n",
        "\n",
        "model.add( Dense(256 , activation='relu') )\n",
        "model.add( Dense(len(chars), activation='softmax') )\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 40, 64)            31232     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 40, 64)            256       \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 64)                33024     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 57)                14649     \n",
            "=================================================================\n",
            "Total params: 96,313\n",
            "Trainable params: 95,929\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nunw2u7n8P28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(80):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hDVWwkx8jmW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=100,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
